"""
FastAPI ä¸»åº”ç”¨
"""
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from .config.settings import settings
from .routes import chat, range
from .models.schemas import HealthResponse
from .services.llm_service import llm_service
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=getattr(logging, settings.log_level),
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

logger = logging.getLogger(__name__)

# åˆ›å»º FastAPI åº”ç”¨
app = FastAPI(
    title="å¾·å·æ‰‘å…‹ AI åŠ©æ‰‹ API",
    description="åŸºäº LangGraph çš„å¾·å·æ‰‘å…‹æ‰‹ç‰ŒèŒƒå›´åˆ†æå’Œæ™ºèƒ½å¯¹è¯ç³»ç»Ÿ",
    version="1.0.0"
)

# é…ç½® CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        settings.frontend_url,
        "http://localhost:3000",
        "http://127.0.0.1:3000"
    ],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# æ³¨å†Œè·¯ç”±
app.include_router(chat.router)
app.include_router(range.router)


@app.on_event("startup")
async def startup_event():
    """åº”ç”¨å¯åŠ¨äº‹ä»¶"""
    logger.info("ğŸš€ å¾·å·æ‰‘å…‹ AI åŠ©æ‰‹å¯åŠ¨ä¸­...")
    logger.info(f"ğŸ“ ç¯å¢ƒ: {settings.environment}")
    logger.info(f"ğŸŒ å‰ç«¯åœ°å€: {settings.frontend_url}")
    
    if llm_service.is_available():
        logger.info(f"âœ… AI æœåŠ¡å·²å¯ç”¨: {llm_service.provider}")
    else:
        logger.warning("âš ï¸  AI æœåŠ¡æœªé…ç½®ï¼Œè¯·æ£€æŸ¥ç¯å¢ƒå˜é‡")
        logger.warning("ğŸ’¡ è¯·å‚è€ƒ .env.example é…ç½® Azure OpenAI æˆ– OpenAI API")


@app.get("/", response_model=HealthResponse)
async def root():
    """
    æ ¹è·¯å¾„ - å¥åº·æ£€æŸ¥
    
    Returns:
        æœåŠ¡çŠ¶æ€
    """
    return HealthResponse(
        status="healthy",
        ai_enabled=llm_service.is_available(),
        ai_provider=llm_service.provider,
        version="1.0.0"
    )


@app.get("/health", response_model=HealthResponse)
async def health_check():
    """
    å¥åº·æ£€æŸ¥æ¥å£
    
    Returns:
        æœåŠ¡çŠ¶æ€
    """
    return HealthResponse(
        status="healthy",
        ai_enabled=llm_service.is_available(),
        ai_provider=llm_service.provider,
        version="1.0.0"
    )


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        "app.main:app",
        host="0.0.0.0",
        port=settings.port,
        reload=settings.environment == "development"
    )

