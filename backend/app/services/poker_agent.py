"""
å¾·å·æ‰‘å…‹ AI åŠ©æ‰‹
ä½¿ç”¨ LangGraph æ„å»ºå¯¹è¯æµç¨‹
"""
from typing import TypedDict, Annotated, List, Dict, Any
from langgraph.graph import StateGraph, END
from langchain_core.messages import HumanMessage, AIMessage
from .llm_service import llm_service
import logging

logger = logging.getLogger(__name__)


class AgentState(TypedDict):
    """Agent çŠ¶æ€"""
    messages: List[Dict[str, str]]
    range_context: Dict[str, Any]
    analysis_result: str


class PokerAgent:
    """å¾·å·æ‰‘å…‹ AI åŠ©æ‰‹"""
    
    def __init__(self):
        self.graph = self._build_graph()
    
    def _build_graph(self) -> StateGraph:
        """æ„å»º LangGraph å·¥ä½œæµ"""
        workflow = StateGraph(AgentState)
        
        # æ·»åŠ èŠ‚ç‚¹
        workflow.add_node("analyze_intent", self.analyze_intent)
        workflow.add_node("answer_question", self.answer_question)
        workflow.add_node("analyze_range", self.analyze_range)
        workflow.add_node("recommend_range", self.recommend_range)
        
        # è®¾ç½®å…¥å£
        workflow.set_entry_point("analyze_intent")
        
        # æ·»åŠ æ¡ä»¶è¾¹
        workflow.add_conditional_edges(
            "analyze_intent",
            self.route_intent,
            {
                "question": "answer_question",
                "analyze": "analyze_range",
                "recommend": "recommend_range",
            }
        )
        
        # æ‰€æœ‰èŠ‚ç‚¹éƒ½è¿æ¥åˆ° END
        workflow.add_edge("answer_question", END)
        workflow.add_edge("analyze_range", END)
        workflow.add_edge("recommend_range", END)
        
        return workflow.compile()
    
    async def analyze_intent(self, state: AgentState) -> AgentState:
        """åˆ†æç”¨æˆ·æ„å›¾"""
        last_message = state["messages"][-1]["content"]
        
        # ç®€å•çš„æ„å›¾è¯†åˆ«ï¼ˆå®é™…å¯ä»¥ç”¨ LLM åšæ›´æ™ºèƒ½çš„è¯†åˆ«ï¼‰
        if "åˆ†æ" in last_message or "è¯„ä»·" in last_message or "åˆç†" in last_message:
            state["intent"] = "analyze"
        elif "æ¨è" in last_message or "å»ºè®®" in last_message or "åº”è¯¥" in last_message:
            state["intent"] = "recommend"
        else:
            state["intent"] = "question"
        
        return state
    
    def route_intent(self, state: AgentState) -> str:
        """è·¯ç”±åˆ°ä¸åŒçš„å¤„ç†èŠ‚ç‚¹"""
        return state.get("intent", "question")
    
    async def answer_question(self, state: AgentState) -> AgentState:
        """å›ç­”ä¸€èˆ¬é—®é¢˜"""
        last_message = state["messages"][-1]["content"]
        range_context = state.get("range_context", {})
        
        # å‡†å¤‡ä¸Šä¸‹æ–‡
        history = state["messages"][:-1][-5:]  # æœ€è¿‘5æ¡æ¶ˆæ¯
        
        # ç”ŸæˆåŒ…å«èŒƒå›´ä¿¡æ¯çš„ system prompt
        system_prompt = llm_service.get_system_prompt(range_context=range_context)
        
        # è°ƒç”¨ LLM
        response = await llm_service.chat(
            message=last_message,
            system_prompt=system_prompt,
            history=history
        )
        
        state["analysis_result"] = response
        return state
    
    async def analyze_range(self, state: AgentState) -> AgentState:
        """åˆ†ææ‰‹ç‰ŒèŒƒå›´"""
        last_message = state["messages"][-1]["content"]
        range_context = state.get("range_context", {})
        
        # æ„å»ºåˆ†ææç¤º
        prompt = f"""è¯·åˆ†æä»¥ä¸‹æ‰‹ç‰ŒèŒƒå›´ï¼š

{last_message}

è¯·ä»ä»¥ä¸‹æ–¹é¢è¿›è¡Œåˆ†æï¼š
1. èŒƒå›´çš„ç´§æ¾ç¨‹åº¦
2. èŒƒå›´çš„å¹³è¡¡æ€§ï¼ˆä»·å€¼ç‰Œå’Œè¯ˆå”¬ç‰Œçš„æ¯”ä¾‹ï¼‰
3. é’ˆå¯¹ä¸åŒå¯¹æ‰‹çš„é€‚ç”¨æ€§
4. å¯èƒ½çš„æ”¹è¿›å»ºè®®"""
        
        # ç”ŸæˆåŒ…å«èŒƒå›´ä¿¡æ¯çš„ system prompt
        system_prompt = llm_service.get_system_prompt(range_context=range_context)
        
        response = await llm_service.chat(
            message=prompt,
            system_prompt=system_prompt
        )
        
        state["analysis_result"] = response
        return state
    
    async def recommend_range(self, state: AgentState) -> AgentState:
        """æ¨èæ‰‹ç‰ŒèŒƒå›´"""
        last_message = state["messages"][-1]["content"]
        range_context = state.get("range_context", {})
        
        prompt = f"""è¯·æ ¹æ®ä»¥ä¸‹ä¿¡æ¯æ¨èåˆé€‚çš„æ‰‹ç‰ŒèŒƒå›´ï¼š

{last_message}

è¯·æä¾›ï¼š
1. æ¨èçš„å…·ä½“æ‰‹ç‰Œåˆ—è¡¨
2. é¢„æœŸçš„èŒƒå›´æ¦‚ç‡
3. æ¨èç†ç”±
4. ä½¿ç”¨æ³¨æ„äº‹é¡¹"""
        
        # ç”ŸæˆåŒ…å«èŒƒå›´ä¿¡æ¯çš„ system prompt
        system_prompt = llm_service.get_system_prompt(range_context=range_context)
        
        response = await llm_service.chat(
            message=prompt,
            system_prompt=system_prompt
        )
        
        state["analysis_result"] = response
        return state
    
    async def chat(
        self, 
        message: str, 
        conversation_history: List[Dict[str, str]] = None,
        range_context: Dict[str, Any] = None
    ) -> str:
        """
        ä¸ AI åŠ©æ‰‹å¯¹è¯ï¼ˆéæµå¼ï¼‰
        
        Args:
            message: ç”¨æˆ·æ¶ˆæ¯
            conversation_history: å¯¹è¯å†å²
            range_context: èŒƒå›´ä¸Šä¸‹æ–‡
        
        Returns:
            AI å›å¤
        """
        if not llm_service.is_available():
            return "æŠ±æ­‰ï¼ŒAI æœåŠ¡å½“å‰ä¸å¯ç”¨ã€‚è¯·æ£€æŸ¥é…ç½®ã€‚"
        
        try:
            # å‡†å¤‡çŠ¶æ€
            messages = conversation_history or []
            messages.append({"role": "user", "content": message})
            
            state = {
                "messages": messages,
                "range_context": range_context or {},
                "analysis_result": ""
            }
            
            # è¿è¡Œå·¥ä½œæµ
            result = await self.graph.ainvoke(state)
            
            return result.get("analysis_result", "æŠ±æ­‰ï¼Œæ— æ³•ç”Ÿæˆå›å¤ã€‚")
            
        except Exception as e:
            logger.error(f"âŒ Agent å¤„ç†å¤±è´¥: {e}")
            return f"æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¯·æ±‚æ—¶å‡ºç°é”™è¯¯: {str(e)}"
    
    async def chat_stream(
        self, 
        message: str, 
        conversation_history: List[Dict[str, str]] = None,
        range_context: Dict[str, Any] = None
    ):
        """
        ä¸ AI åŠ©æ‰‹å¯¹è¯ï¼ˆæµå¼ï¼‰
        
        Args:
            message: ç”¨æˆ·æ¶ˆæ¯
            conversation_history: å¯¹è¯å†å²
            range_context: èŒƒå›´ä¸Šä¸‹æ–‡
        
        Yields:
            AI å›å¤çš„æ–‡æœ¬å—
        """
        if not llm_service.is_available():
            yield "æŠ±æ­‰ï¼ŒAI æœåŠ¡å½“å‰ä¸å¯ç”¨ã€‚è¯·æ£€æŸ¥é…ç½®ã€‚"
            return
        
        try:
            # å‡†å¤‡å†å²
            history = conversation_history or []
            
            # ç”ŸæˆåŒ…å«èŒƒå›´ä¿¡æ¯çš„ system prompt
            system_prompt = llm_service.get_system_prompt(range_context=range_context)
            
            # è°ƒè¯•æ—¥å¿—
            logger.info(f"ğŸ“Š Range Context: {range_context}")
            logger.info(f"ğŸ“ System Prompt Length: {len(system_prompt)}")
            if range_context:
                logger.info(f"âœ… èŒƒå›´ä¿¡æ¯å·²æ³¨å…¥: {range_context.get('name', 'Unknown')}")
            
            # ç›´æ¥ä½¿ç”¨ LLM æœåŠ¡çš„æµå¼æ–¹æ³•ï¼ˆç®€åŒ–ç‰ˆï¼‰
            async for chunk in llm_service.chat_stream(
                message=message,
                system_prompt=system_prompt,
                history=history[-5:]  # æœ€è¿‘5æ¡æ¶ˆæ¯
            ):
                yield chunk
            
        except Exception as e:
            logger.error(f"âŒ æµå¼ Agent å¤„ç†å¤±è´¥: {e}")
            yield f"æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¯·æ±‚æ—¶å‡ºç°é”™è¯¯: {str(e)}"


# å…¨å±€ Agent å®ä¾‹
poker_agent = PokerAgent()

